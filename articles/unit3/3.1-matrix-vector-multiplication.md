---
layout: default
title: "Matrix-Vector Multiplication"
---

# Matrix-Vector Multiplication

_What does it mean to multiply a matrix by a vector?_

---

## Two Perspectives

We've already seen matrix-vector multiplication in disguise. When we wrote:

$$
x_1 \begin{bmatrix} 2 \\ 1 \end{bmatrix} + x_2 \begin{bmatrix} 3 \\ 4 \end{bmatrix} = \begin{bmatrix} 11 \\ 9 \end{bmatrix}
$$

This is exactly the same as:

$$
\begin{bmatrix} 2 & 3 \\ 1 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 11 \\ 9 \end{bmatrix}
$$

Matrix-vector multiplication is just a compact way of writing a linear combination of columns.

But there's also a row-by-row way to think about it. Let's explore both.

---

## Method 1: Linear Combination of Columns

$$
A\mathbf{x} = x_1 (\text{column 1}) + x_2 (\text{column 2}) + \cdots + x_n (\text{column } n)
$$

### Example

$$
\begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix}
= 2 \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix} + 3 \begin{bmatrix} 2 \\ 4 \\ 6 \end{bmatrix}
= \begin{bmatrix} 2 \\ 6 \\ 10 \end{bmatrix} + \begin{bmatrix} 6 \\ 12 \\ 18 \end{bmatrix}
= \begin{bmatrix} 8 \\ 18 \\ 28 \end{bmatrix}
$$

The entries of $\mathbf{x}$ are the weights in a linear combination of $A$'s columns.

<video src="/linear_algebra_KA/videos/MatVecAsLinearCombo.mp4" autoplay loop muted playsinline style="max-width:100%; border-radius:8px;"></video>
_Animation showing columns of A being scaled by entries of x, then added together visually._

---

## Method 2: Row-by-Row (Dot Products)

Each entry of the result is the **dot product** of a row of $A$ with the vector $\mathbf{x}$.

$$
(A\mathbf{x})_i = (\text{row } i \text{ of } A) \cdot \mathbf{x}
$$

### Example

$$
\begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \end{bmatrix}
= \begin{bmatrix} 1(2) + 2(3) \\ 3(2) + 4(3) \\ 5(2) + 6(3) \end{bmatrix}
= \begin{bmatrix} 8 \\ 18 \\ 28 \end{bmatrix}
$$

Same answer, different path.

### When to Use Which?

- **Column method:** Great for understanding the geometry. $A\mathbf{x}$ asks "what linear combination of columns does $\mathbf{x}$ specify?"
- **Row method:** Great for computation. Just do the dot products.

Both methods always give the same result.

---

**Practice Problem:** Compute the following using the row method:

$$\begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & 3 \end{bmatrix} \begin{bmatrix} 4 \\ 5 \\ 1 \end{bmatrix}$$

$$\text{(A) } \begin{bmatrix} 6 \\ 8 \end{bmatrix} \qquad \text{(B) } \begin{bmatrix} 4 \\ 5 \end{bmatrix} \qquad \text{(C) } \begin{bmatrix} 6 \\ 8 \\ 2 \end{bmatrix} \qquad \text{(D) } \begin{bmatrix} 7 \\ 9 \end{bmatrix}$$

<details>
<summary><strong>Check your answer</strong></summary>

**(A)**

$$\begin{bmatrix} 6 \\ 8 \end{bmatrix}$$

Row 1: $1(4) + 0(5) + 2(1) = 4 + 0 + 2 = 6$

Row 2: $0(4) + 1(5) + 3(1) = 0 + 5 + 3 = 8$

</details>

---

## Size Matters

For $A\mathbf{x}$ to make sense:

- **$A$ must have as many columns as $\mathbf{x}$ has entries**

If $A$ is $m \times n$ (m rows, n columns), then $\mathbf{x}$ must have $n$ entries, and the result has $m$ entries.

$$
\underbrace{A}_{m \times n} \cdot \underbrace{\mathbf{x}}_{n \times 1} = \underbrace{A\mathbf{x}}_{m \times 1}
$$

![Dimension matching](/linear_algebra_KA/images/dimension_matching.png)
_Visual showing dimensions: the "inner dimensions" (columns of A, rows of x) must match._

### Example of Dimension Mismatch

$$
\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \text{undefined!}
$$

The matrix has 3 columns, but the vector has 2 entries. You can't do the multiplication.

---

**Practice Problem:** A matrix $A$ is $4 \times 5$. Which vector can you multiply it by?

- (A) $\mathbf{x}$ in ℝ⁴
- (B) $\mathbf{x}$ in ℝ⁵
- (C) $\mathbf{x}$ in ℝ⁹
- (D) Any of the above

<details>
<summary><strong>Check your answer</strong></summary>

**(B) $\mathbf{x}$ in ℝ⁵**

A 4×5 matrix has 5 columns, so it needs a vector with 5 entries. The result will have 4 entries.

</details>

---

## Properties of Matrix-Vector Multiplication

Matrix-vector multiplication is **linear**. This is the whole point.

For any matrix $A$, vectors $\mathbf{u}, \mathbf{v}$, and scalar $c$:

$$
A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}
$$

$$
A(c\mathbf{u}) = c(A\mathbf{u})
$$

These properties say: "It doesn't matter if you add/scale first and then multiply, or multiply first and then add/scale."

This is exactly what makes the transformation $\mathbf{x} \mapsto A\mathbf{x}$ a **linear transformation** — the subject of our next section.

<details>
<summary><strong>Why are these properties true?</strong></summary>

Both follow from the column definition. For the first property:

$$A(\mathbf{u} + \mathbf{v}) = \sum_j (u_j + v_j) \mathbf{a}_j = \sum_j u_j \mathbf{a}_j + \sum_j v_j \mathbf{a}_j = A\mathbf{u} + A\mathbf{v}$$

It's just rearranging sums.

</details>

---

## The Identity Matrix

There's a special matrix that does nothing:

$$
I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \quad \text{(in 2D)}
\qquad
I = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad \text{(in 3D)}
$$

The **identity matrix** has 1s on the diagonal and 0s everywhere else.

$$
I\mathbf{x} = \mathbf{x} \quad \text{for any } \mathbf{x}
$$

It's the matrix equivalent of multiplying by 1.

---

## Connecting to Systems of Equations

We can now write any system of linear equations as:

$$
A\mathbf{x} = \mathbf{b}
$$

Where:
- $A$ is the coefficient matrix
- $\mathbf{x}$ is the vector of unknowns
- $\mathbf{b}$ is the vector of constants

This compact notation unifies everything. Solving the system means finding which $\mathbf{x}$ produces $\mathbf{b}$ when transformed by $A$.

---

**Practice Problem:** Write the system below as a matrix equation $A\mathbf{x} = \mathbf{b}$:

$$
\begin{aligned}
2x - y &= 5 \\
x + 3y &= 7
\end{aligned}
$$

Which is the correct matrix $A$?

$$\text{(A) } \begin{bmatrix} 2 & 1 \\ -1 & 3 \end{bmatrix} \qquad \text{(B) } \begin{bmatrix} 2 & -1 \\ 1 & 3 \end{bmatrix} \qquad \text{(C) } \begin{bmatrix} 5 & 7 \\ 2 & 1 \end{bmatrix} \qquad \text{(D) } \begin{bmatrix} x & y \\ 2 & -1 \end{bmatrix}$$

<details>
<summary><strong>Check your answer</strong></summary>

**(B)**

$$\begin{bmatrix} 2 & -1 \\ 1 & 3 \end{bmatrix}$$

The full equation is:
$$\begin{bmatrix} 2 & -1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 5 \\ 7 \end{bmatrix}$$

The coefficients go in $A$, the unknowns in $\mathbf{x}$, and the constants in $\mathbf{b}$.

</details>

---

## Summary

- **Matrix-vector multiplication** combines the columns of $A$ using entries of $\mathbf{x}$ as weights
- **Column view:** $A\mathbf{x}$ is a linear combination of $A$'s columns
- **Row view:** Each entry of $A\mathbf{x}$ is a dot product of a row with $\mathbf{x}$
- **Dimension rule:** $(m \times n) \cdot (n \times 1) = (m \times 1)$
- **Linearity:** $A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}$ and $A(c\mathbf{u}) = c(A\mathbf{u})$
- **Identity matrix:** $I\mathbf{x} = \mathbf{x}$ for all $\mathbf{x}$
- **Systems:** $A\mathbf{x} = \mathbf{b}$ is the compact form of a linear system

---

## What's Next?

Matrix-vector multiplication takes an input vector and produces an output vector. This is a **function** from ℝⁿ to ℝᵐ.

But it's a special kind of function — one that preserves addition and scalar multiplication. In the next section, we'll explore these **linear transformations** and see how they warp and reshape space.


---

[← Previous: Linear Independence](../unit2/2.3-linear-independence.md) | [Back to Home](https://ilundholm.github.io/linear_algebra_KA/) | [Next: Linear Transformations →](3.2-linear-transformations.md)
