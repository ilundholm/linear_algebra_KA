---
layout: article
title: "Linear Transformations"
---



_Matrices as functions that warp space._

---

## A New Perspective

So far, we've thought of matrices as:
- Tables of coefficients (for systems of equations)
- Collections of column vectors (for span and linear combinations)

Now we add a third perspective: **a matrix is a function**.

The function $T(\mathbf{x}) = A\mathbf{x}$ takes an input vector $\mathbf{x}$ and produces an output vector $A\mathbf{x}$.

$$
T: \mathbb{R}^n \to \mathbb{R}^m
$$

We call this a **linear transformation** from ℝⁿ to ℝᵐ.

---

## What Makes It "Linear"?

A transformation $T$ is linear if it satisfies two properties:

**1. Additivity:** $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$

**2. Homogeneity:** $T(c\mathbf{u}) = cT(\mathbf{u})$

In plain English:
- If you add two vectors and then transform, you get the same result as transforming each and then adding
- If you scale a vector and then transform, you get the same result as transforming and then scaling

Matrix multiplication automatically satisfies both properties. Every matrix defines a linear transformation.

<details>
<summary><strong>Why does this matter?</strong></summary>

Linearity means the transformation "respects" the structure of vector operations. This makes linear transformations predictable and analyzable.

Nonlinear transformations (like squaring each component) are much harder to work with because they break these nice properties.

</details>

---

## Visualizing Transformations in 2D

The best way to understand linear transformations is to watch them in action.

<iframe src="https://ilundholm.github.io/linear_algebra_KA/widgets/transformation-grid.html" width="100%" height="550" style="border: none; border-radius: 8px;"></iframe>

### What Happens to the Grid?

Start with a regular grid of lines. Apply a linear transformation. What do you observe?

**Key fact:** Linear transformations:
- Keep the origin fixed
- Turn straight lines into straight lines
- Turn parallel lines into parallel lines

The grid might stretch, rotate, shear, or flip — but it stays a grid.

<video src="/linear_algebra_KA/videos/TransformationsOnGrid.mp4" autoplay loop muted playsinline style="max-width:100%; border-radius:8px;"></video>
_Animation showing several 2D transformations: scaling, rotation, shear, reflection. Each time, show the grid warping._

---

## Standard Transformations

Here are some common 2D linear transformations and their matrices:

### Scaling

Stretch or shrink along the axes:

$$
\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}
$$

Scales $x$ by 2 and $y$ by 3.

![Unit square becoming a 2×3 rectangle under scaling](/linear_algebra_KA/images/unit_square_scaled.png)
_Before/after showing unit square becoming a 2×3 rectangle._

### Rotation

Rotate counterclockwise by angle $\theta$:

$$
\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
$$

For 90° counterclockwise ($\theta = 90°$):

$$
\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
$$

### Reflection

Reflect across the $x$-axis:

$$
\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
$$

Reflect across the line $y = x$:

$$
\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
$$

### Shear

Horizontal shear (tilt):

$$
\begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}
$$

This slides points horizontally by an amount proportional to their $y$-coordinate.

### Projection

Project onto the $x$-axis:

$$
\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
$$

Every point gets squished down to the $x$-axis.

---

**Practice Problem:** What transformation does this matrix perform?

$$
\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
$$

- (A) Rotation by 90° counterclockwise
- (B) Rotation by 90° clockwise
- (C) Reflection across $y = x$
- (D) Horizontal shear

<details>
<summary><strong>Check your answer</strong></summary>

**(B) Rotation by 90° clockwise**

Let's check where the standard basis vectors go:
- $\begin{bmatrix} 1 \\ 0 \end{bmatrix} \to \begin{bmatrix} 0 \\ -1 \end{bmatrix}$ (right goes to down)
- $\begin{bmatrix} 0 \\ 1 \end{bmatrix} \to \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ (up goes to right)

That's clockwise rotation.

(Counterclockwise 90° would be $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$.)

</details>

---

## The Key Insight: Columns Are Images of Basis Vectors

Here's the most important observation about linear transformations:

> **The columns of a matrix tell you where the standard basis vectors land.**

For a 2×2 matrix:
$$
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
$$

- Column 1 $\begin{bmatrix} a \\ c \end{bmatrix}$ is where $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ (the $x$-direction) goes
- Column 2 $\begin{bmatrix} b \\ d \end{bmatrix}$ is where $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ (the $y$-direction) goes

Once you know where the basis vectors land, linearity tells you where *everything* lands.

<video src="/linear_algebra_KA/videos/BasisVectorsToColumns.mp4" autoplay loop muted playsinline style="max-width:100%; border-radius:8px;"></video>
_Animation: Start with basis vectors. Apply transformation. Show that the columns of the matrix are exactly where the basis vectors end up._

### Example

Suppose a transformation sends:
- $\begin{bmatrix} 1 \\ 0 \end{bmatrix} \to \begin{bmatrix} 3 \\ 1 \end{bmatrix}$
- $\begin{bmatrix} 0 \\ 1 \end{bmatrix} \to \begin{bmatrix} 2 \\ 4 \end{bmatrix}$

Then the matrix is:

$$
A = \begin{bmatrix} 3 & 2 \\ 1 & 4 \end{bmatrix}
$$

That's it! The images of the basis vectors *are* the columns.

---

**Practice Problem:** A linear transformation $T$ satisfies:
- $T\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\right) = \begin{bmatrix} 5 \\ -2 \end{bmatrix}$
- $T\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}\right) = \begin{bmatrix} 0 \\ 3 \end{bmatrix}$

What is the matrix of $T$?

$$\text{(A) } \begin{bmatrix} 5 & -2 \\ 0 & 3 \end{bmatrix} \qquad \text{(B) } \begin{bmatrix} 5 & 0 \\ -2 & 3 \end{bmatrix} \qquad \text{(C) } \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \qquad \text{(D) } \begin{bmatrix} 5 & 0 \\ 0 & 3 \end{bmatrix}$$

<details>
<summary><strong>Check your answer</strong></summary>

**(B)**

$$\begin{bmatrix} 5 & 0 \\ -2 & 3 \end{bmatrix}$$

The images of the basis vectors become the columns:

$$\text{First column: } \begin{bmatrix} 5 \\ -2 \end{bmatrix} \qquad \text{Second column: } \begin{bmatrix} 0 \\ 3 \end{bmatrix}$$

</details>

---

## Every Linear Transformation Has a Matrix

This is a fundamental theorem:

> **Every linear transformation from ℝⁿ to ℝᵐ can be represented by an $m \times n$ matrix.**

To find the matrix:
1. Apply the transformation to each standard basis vector
2. Use the outputs as columns

There's no other kind of linear transformation — matrices capture them all.

<details>
<summary><strong>What about infinite dimensions?</strong></summary>

In infinite-dimensional vector spaces (like spaces of functions), the relationship between linear transformations and matrices is more subtle. But in ℝⁿ, the correspondence is perfect.

</details>

---

## Domain and Codomain

The **domain** of $T(\mathbf{x}) = A\mathbf{x}$ is ℝⁿ (where inputs live).

The **codomain** is ℝᵐ (where outputs could potentially live).

The **range** (or image) is the set of vectors that *actually* get hit — which is the span of $A$'s columns!

| Concept | For $T: \mathbb{R}^n \to \mathbb{R}^m$ |
|---------|---------------------------------------|
| Domain | ℝⁿ (all possible inputs) |
| Codomain | ℝᵐ (space where outputs live) |
| Range | Span of columns (actual outputs) |

---

**Practice Problem:** If $A$ is a $3 \times 5$ matrix, what can you say about the transformation $T(\mathbf{x}) = A\mathbf{x}$?

- (A) It maps ℝ³ to ℝ⁵
- (B) It maps ℝ⁵ to ℝ³
- (C) It maps ℝ⁵ to ℝ⁵
- (D) It maps ℝ³ to ℝ³

<details>
<summary><strong>Check your answer</strong></summary>

**(B) It maps ℝ⁵ to ℝ³**

A $3 \times 5$ matrix has 5 columns (so inputs have 5 entries) and 3 rows (so outputs have 3 entries).

$T: \mathbb{R}^5 \to \mathbb{R}^3$

</details>

---

## Geometric Interpretation in Higher Dimensions

In 3D, linear transformations do similar things:
- Scaling, rotation, reflection, shear
- Projecting 3D space onto a plane
- Stretching along certain directions

And the same principle applies: the columns of a $3 \times 3$ matrix tell you where the three standard basis vectors go.

<iframe src="https://ilundholm.github.io/linear_algebra_KA/widgets/transformation-3d.html" width="100%" height="550" style="border: none; border-radius: 8px;"></iframe>
_Interactive 3D transform: visualize how the basis vectors (colored arrows) determine the transformation._

---

## Summary

- A **linear transformation** is a function $T: \mathbb{R}^n \to \mathbb{R}^m$ that preserves addition and scalar multiplication
- Every linear transformation from ℝⁿ to ℝᵐ is given by an $m \times n$ matrix
- **The columns of the matrix are the images of the standard basis vectors**
- Linear transformations turn grids into grids (origin fixed, lines stay lines)
- Common 2D transformations: scaling, rotation, reflection, shear, projection
- The **range** of a transformation is the span of the matrix columns

---

## What's Next?

We've seen that every matrix defines a linear transformation, and every linear transformation has a matrix. But how do we *find* that matrix?

In the next section, we'll develop a systematic method: **just watch where the basis vectors go**. This simple idea lets us write down the matrix for any geometric transformation — rotations, reflections, projections, and more — without any guesswork.


---

[← Previous: Matrix-Vector Multiplication](3.1-matrix-vector-multiplication.md) | [Back to Home](https://ilundholm.github.io/linear_algebra_KA/) | [Next: Matrix of a Transformation →](3.3-matrix-of-transformation.md)
