---
layout: default
title: "Linear Independence"
---

# Linear Independence

_How do you know when a vector is redundant?_

---

## The Key Question

We've seen that some sets of vectors span more "space" than others:
- One vector spans a line
- Two non-parallel vectors span a plane
- Three non-coplanar vectors span 3D space

But what makes vectors "non-parallel" or "non-coplanar"? When do vectors actually contribute something new to the span?

This is the question of **linear independence**.

---

## The Definition

A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ is **linearly independent** if the only way to make the zero vector as a linear combination is to use all zero coefficients:

$$
c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k = \mathbf{0} \implies c_1 = c_2 = \cdots = c_k = 0
$$

If there's a *nontrivial* way to get zero (some $c_i \neq 0$), the vectors are **linearly dependent**.

<details>
<summary><strong>Why this definition?</strong></summary>

If you can write:
$$c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}$$
with, say, $c_3 \neq 0$, then you can rearrange:
$$\mathbf{v}_3 = -\frac{c_1}{c_3}\mathbf{v}_1 - \frac{c_2}{c_3}\mathbf{v}_2$$

This means $\mathbf{v}_3$ is a linear combination of the others — it's redundant! It doesn't add anything new to the span.

</details>

---

## Geometric Intuition

### In ℝ²

**Two vectors are linearly dependent** if and only if they're parallel (one is a scalar multiple of the other).

**[PLACEHOLDER: Static image]**
_Two parallel vectors (dependent) vs. two non-parallel vectors (independent)._

If $\mathbf{v}_2 = 3\mathbf{v}_1$, then:
$$(-3)\mathbf{v}_1 + (1)\mathbf{v}_2 = \mathbf{0}$$

That's a nontrivial way to make zero, so they're dependent.

### In ℝ³

**Three vectors are linearly dependent** if and only if they're coplanar (all lie in the same plane through the origin).

**[PLACEHOLDER: Video ~45s]**
_3D visualization: Three coplanar vectors (one can be built from the other two). Then three non-coplanar vectors (point in genuinely different directions)._

If all three lie in a plane, the third vector can be expressed as a linear combination of the first two — it's redundant.

---

**Practice Problem:** Are the vectors $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\begin{bmatrix} -3 \\ -6 \end{bmatrix}$ linearly independent or dependent?

- (A) Independent
- (B) Dependent

<details>
<summary><strong>Check your answer</strong></summary>

**(B) Dependent**

Notice that $\begin{bmatrix} -3 \\ -6 \end{bmatrix} = -3 \begin{bmatrix} 1 \\ 2 \end{bmatrix}$.

So $(-3)\mathbf{v}_1 + (1)\mathbf{v}_2 = \mathbf{0}$ with not-all-zero coefficients. They're parallel and thus dependent.

</details>

---

## Testing for Independence: Row Reduction

Here's the practical method: to test if $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ are linearly independent:

1. Form a matrix with these vectors as **columns**
2. Row reduce to echelon form
3. Count the pivots

**If there's a pivot in every column**, the vectors are linearly independent.

**If any column lacks a pivot**, the vectors are linearly dependent.

### Why does this work?

We're checking if there's a nontrivial solution to:

$$
c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k = \mathbf{0}
$$

This is the homogeneous system with matrix $A = [\mathbf{v}_1 \; \mathbf{v}_2 \; \cdots \; \mathbf{v}_k]$.

- Pivot in every column → no free variables → only the trivial solution → independent
- Missing pivot → free variable → nontrivial solutions → dependent

---

### Example: Testing Three Vectors

Are these vectors linearly independent?

$$
\mathbf{v}_1 = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}, \quad
\mathbf{v}_2 = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}, \quad
\mathbf{v}_3 = \begin{bmatrix} 3 \\ 3 \\ 1 \end{bmatrix}
$$

**Step 1: Form the matrix**

$$
A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \\ 1 & 0 & 1 \end{bmatrix}
$$

**Step 2: Row reduce**

<details>
<summary><strong>Click to see the row reduction</strong></summary>

$$
\begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 3 \\ 1 & 0 & 1 \end{bmatrix}
\xrightarrow[R_3 - R_1]{R_2 - 2R_1}
\begin{bmatrix} 1 & 2 & 3 \\ 0 & -3 & -3 \\ 0 & -2 & -2 \end{bmatrix}
$$

$$
\xrightarrow{R_3 - \frac{2}{3}R_2}
\begin{bmatrix} 1 & 2 & 3 \\ 0 & -3 & -3 \\ 0 & 0 & 0 \end{bmatrix}
$$

</details>

**Step 3: Count pivots**

The echelon form has pivots in columns 1 and 2, but **not in column 3**.

**Conclusion:** The vectors are **linearly dependent**. 

The third vector can be written as a linear combination of the first two. (Specifically, $\mathbf{v}_3 = \mathbf{v}_1 + \mathbf{v}_2$.)

---

**Practice Problem:** You row reduce a matrix and get:

$$
\begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \end{bmatrix}
$$

The original columns are:

- (A) Linearly independent
- (B) Linearly dependent

<details>
<summary><strong>Check your answer</strong></summary>

**(B) Linearly dependent**

Column 3 has no pivot. This means there's a free variable, so the homogeneous system has nontrivial solutions. The third column is a linear combination of the first two.

</details>

---

## Important Facts About Linear Independence

### Fact 1: More vectors than dimensions → always dependent

If you have more vectors than the dimension of the space, they *must* be linearly dependent.

- 3 vectors in ℝ² → dependent (can't all point in "new" directions)
- 5 vectors in ℝ³ → dependent
- $k$ vectors in ℝⁿ with $k > n$ → dependent

Why? The matrix has more columns than rows, so there can't be a pivot in every column.

### Fact 2: The zero vector makes everything dependent

If one of your vectors is $\mathbf{0}$, the set is automatically dependent.

$$
1 \cdot \mathbf{0} + 0 \cdot \mathbf{v}_1 + 0 \cdot \mathbf{v}_2 + \cdots = \mathbf{0}
$$

That's a nontrivial combination (the first coefficient is 1, not 0).

### Fact 3: Two vectors are dependent iff parallel

In any dimension, two vectors are linearly dependent if and only if one is a scalar multiple of the other.

---

## Connection to Earlier Concepts

Linear independence ties together ideas from Unit 1:

| Concept | Translation |
|---------|-------------|
| Vectors are linearly independent | Homogeneous system has only trivial solution |
| Vectors are linearly dependent | Homogeneous system has free variables |
| Columns span ℝᵐ | System $A\mathbf{x} = \mathbf{b}$ has a solution for every $\mathbf{b}$ |
| Columns are independent AND span | System has exactly one solution for every $\mathbf{b}$ |

The last row is special: when your columns are independent *and* span the whole space, you have exactly the right number of vectors — no redundancy, no gaps. This is connected to the idea of a **basis**, which we'll explore in a later unit.

---

**Practice Problem:** A 4×4 matrix has 4 pivot positions. What can you conclude about its columns?

- (A) They are linearly independent
- (B) They are linearly dependent
- (C) They span ℝ⁴
- (D) Both (A) and (C)

<details>
<summary><strong>Check your answer</strong></summary>

**(D) Both (A) and (C)**

4 pivots in a 4×4 matrix means:
- A pivot in every column → columns are linearly independent
- A pivot in every row → columns span ℝ⁴

This is the "perfect" case where the matrix is invertible.

</details>

---

## The "Too Many Vectors" Intuition

Here's a useful way to think about it:

- **Independent vectors:** Each one points in a genuinely new direction. None is redundant.
- **Dependent vectors:** At least one is "wasted" — it could be built from the others.

Imagine standing at the origin with a set of vectors:
- Can all of them take you somewhere new?
- Or is one of them just "more of the same"?

**[PLACEHOLDER: Interactive widget]**
_User can add vectors to a set (in 2D or 3D). Widget indicates whether the set is independent or dependent, and highlights the "redundant" vector if dependent._

---

## Summary

- **Linearly independent:** The only way to make $\mathbf{0}$ is with all zero coefficients
- **Linearly dependent:** There's a nontrivial combination that equals $\mathbf{0}$
- **Geometric intuition:** Dependent vectors are "redundant" — they don't point in new directions
- **Test via row reduction:** Pivot in every column → independent; missing pivot → dependent
- **More vectors than dimensions → always dependent**
- **The zero vector → always causes dependence**

---

## What's Next?

We now have the language to describe when vectors are "just right":
- **Span** tells us what we can reach
- **Independence** tells us we have no redundancy

In the next unit, we'll see how matrices *act* on vectors — transforming them, rotating them, stretching them. This is the **transformation picture** of linear algebra, and it's where matrices come alive.


---

[← Previous: Linear Combinations and Span](2.2-linear-combinations-and-span.md) | [Back to Home](/) | [Next: Matrix-Vector Multiplication →](../unit3/3.1-matrix-vector-multiplication.md)
